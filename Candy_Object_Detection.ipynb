{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2d2782c2-f544-4dd8-b96a-eec7b5aa3a0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy<2\n",
      "  Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl.metadata (61 kB)\n",
      "Using cached numpy-1.26.4-cp312-cp312-macosx_11_0_arm64.whl (13.7 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.6\n",
      "    Uninstalling numpy-2.2.6:\n",
      "      Successfully uninstalled numpy-2.2.6\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "opencv-python 4.12.0.88 requires numpy<2.3.0,>=2; python_version >= \"3.9\", but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-1.26.4\n",
      "Requirement already satisfied: ultralytics in /opt/anaconda3/lib/python3.12/site-packages (8.3.167)\n",
      "Requirement already satisfied: opencv-python in /opt/anaconda3/lib/python3.12/site-packages (4.12.0.88)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (2.2.3)\n",
      "Requirement already satisfied: pillow in /opt/anaconda3/lib/python3.12/site-packages (10.4.0)\n",
      "Requirement already satisfied: pyyaml in /opt/anaconda3/lib/python3.12/site-packages (6.0.1)\n",
      "Requirement already satisfied: numpy>=1.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (1.26.4)\n",
      "Requirement already satisfied: matplotlib>=3.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (3.10.1)\n",
      "Requirement already satisfied: requests>=2.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (2.32.3)\n",
      "Requirement already satisfied: scipy>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (1.13.1)\n",
      "Requirement already satisfied: torch>=1.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (2.5.1)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (0.20.1)\n",
      "Requirement already satisfied: tqdm>=4.64.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (4.66.5)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (5.9.0)\n",
      "Requirement already satisfied: py-cpuinfo in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (9.0.0)\n",
      "Requirement already satisfied: ultralytics-thop>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from ultralytics) (2.0.14)\n",
      "Collecting numpy>=1.23.0 (from ultralytics)\n",
      "  Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl.metadata (62 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (4.51.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (1.4.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (24.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from matplotlib>=3.3.0->ultralytics) (3.1.2)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.23.0->ultralytics) (2025.7.14)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (4.11.0)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (75.1.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (1.13.1)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.3)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch>=1.8.0->ultralytics) (2024.6.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch>=1.8.0->ultralytics) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch>=1.8.0->ultralytics) (2.1.3)\n",
      "Using cached numpy-2.2.6-cp312-cp312-macosx_14_0_arm64.whl (5.1 MB)\n",
      "Installing collected packages: numpy\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 1.26.4\n",
      "    Uninstalling numpy-1.26.4:\n",
      "      Successfully uninstalled numpy-1.26.4\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "gensim 4.3.3 requires numpy<2.0,>=1.18.5, but you have numpy 2.2.6 which is incompatible.\n",
      "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed numpy-2.2.6\n",
      "2.2.6\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "import json\n",
    "import yaml\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "from PIL import Image\n",
    "\n",
    "project_root = Path.cwd()\n",
    "print(f\"Working directory: {project_root}\")\n",
    "\n",
    "# Create required directories\n",
    "dirs_to_create = [\n",
    "    \"Images\",\n",
    "    \"Annotation_CSV_Files\",\n",
    "    \"labels\",\n",
    "    \"dataset/images/train\",\n",
    "    \"dataset/images/val\",\n",
    "    \"dataset/labels/train\",\n",
    "    \"dataset/labels/val\",\n",
    "    \"test_results\"\n",
    "]\n",
    "for d in dirs_to_create:\n",
    "    Path(d).mkdir(parents=True, exist_ok=True)\n",
    "print(\"Directory structure created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "add2eb25-9f21-4030-8ce1-8638c00f6bcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: /Users/larasabha/Desktop/Candy-object-detection\n",
      "Directory structure created\n"
     ]
    }
   ],
   "source": [
    "def organize_dataset_files():\n",
    "    \"\"\"\n",
    "    organize files from extracted Kaggle archive structure into project folders\n",
    "    Moves:\n",
    "    - Images -> Images/\n",
    "    - CSV annotation files -> Annotation_CSV_Files/\n",
    "    \"\"\"\n",
    "    print(\"organizing dataset files...\")\n",
    "\n",
    "    archive_folders = [d for d in os.listdir('.') if os.path.isdir(d) and 'candy' in d.lower()]\n",
    "    if archive_folders:\n",
    "        archive_folder = archive_folders[0]\n",
    "        print(f\"Found archive folder: {archive_folder}\")\n",
    "        source_img_folder = os.path.join(archive_folder, \"Images\")\n",
    "        source_csv_folder = os.path.join(archive_folder, \"annotation csv files\")\n",
    "    else:\n",
    "        print(\"no archive folder found\")\n",
    "        source_img_folder = \"Images\"\n",
    "        source_csv_folder = \"annotation csv files\"\n",
    "\n",
    "    moved_images = 0\n",
    "    moved_csvs = 0\n",
    "\n",
    "    if os.path.exists(source_img_folder):\n",
    "        for file in os.listdir(source_img_folder):\n",
    "            if file.lower().endswith(('.jpg', '.jpeg', '.JPG', 'JPEG')):\n",
    "                dst = os.path.join(\"Images\", file)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.move(os.path.join(source_img_folder, file), dst)\n",
    "                    moved_images += 1\n",
    "        print(f\"moved {moved_images} images to Images/\")\n",
    "    else:\n",
    "        print(f\"images folder not found at {source_img_folder}\")\n",
    "\n",
    "    if os.path.exists(source_csv_folder):\n",
    "        for file in os.listdir(source_csv_folder):\n",
    "            if file.endswith('.csv'):\n",
    "                dst = os.path.join(\"Annotation_CSV_Files\", file)\n",
    "                if not os.path.exists(dst):\n",
    "                    shutil.move(os.path.join(source_csv_folder, file), dst)\n",
    "                    moved_csvs += 1\n",
    "        print(f\"moved {moved_csvs} CSV files to Annotation_CSV_Files/\")\n",
    "    else:\n",
    "        print(f\"CSV folder not found at {source_csv_folder}\")\n",
    "\n",
    "    final_images = len([f for f in os.listdir(\"Images\") if f.lower().endswith(('.jpg', '.jpeg', '.png'))])\n",
    "    final_csvs = len([f for f in os.listdir(\"Annotation_CSV_Files\") if f.endswith('.csv')])\n",
    "\n",
    "    print(f\"Final counts: {final_images} images, {final_csvs} CSV files\")\n",
    "    return final_images > 0 and final_csvs > 0\n",
    "\n",
    "organize_dataset_files()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "64690e28-70b0-4887-9725-96741b86f693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "converting CSV annotations to YOLO format...\n",
      "found 11 CSV files\n",
      "loaded Candy_Project_1930-1981_csv.csv\n",
      "loaded Candy_Project_2039-2092_csv.csv\n",
      "loaded Candy_Project_2359-2415_csv.csv\n",
      "loaded Candy_Project_1982-2038_csv.csv\n",
      "loaded Candy_Project_2147-2197_csv.csv\n",
      "loaded Candy_Project_2468-2496_csv.csv\n",
      "loaded Candy_Project_2093-2146_csv.csv\n",
      "loaded Candy_Project_2304-2358_csv.csv\n",
      "loaded Candy_Project_2253-2303_csv.csv\n",
      "loaded Candy_Project_2416-2467_csv.csv\n",
      "loaded Candy_Project_2198-2252_csv.csv\n",
      "total annotations: 2113\n",
      "found 9 classes: ['100_Grand', '3_Musketeers', 'Baby_Ruth', 'Butterfingers', 'Crunch', 'Midnight_Milky_Way', 'Milky_Way', 'Snickers', 'Twix']\n",
      "created classes.txt\n",
      "converted 528 image-label pairs\n"
     ]
    }
   ],
   "source": [
    "def convert_csv_to_yolo(csv_folder=\"annotation CSV files\", \n",
    "                       image_folder=\"Images\", \n",
    "                       output_labels=\"labels\"):\n",
    "    \"\"\"convert CSV annotations to YOLO format\"\"\"\n",
    "    \n",
    "    print(\"converting CSV annotations to YOLO format...\")\n",
    "    \n",
    "    #create output directory\n",
    "    os.makedirs(output_labels, exist_ok=True)\n",
    "    \n",
    "    #check if CSV folder exists\n",
    "    if not os.path.exists(csv_folder):\n",
    "        print(f\"CSV folder not found: {csv_folder}\")\n",
    "        return False\n",
    "    \n",
    "    #combine all CSV files\n",
    "    csv_files = [f for f in os.listdir(csv_folder) if f.endswith(\".csv\")]\n",
    "    if not csv_files:\n",
    "        print(\"no CSV files found\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"found {len(csv_files)} CSV files\")\n",
    "    \n",
    "    df_list = []\n",
    "    for csv_file in csv_files:\n",
    "        try:\n",
    "            df = pd.read_csv(os.path.join(csv_folder, csv_file))\n",
    "            df_list.append(df)\n",
    "            print(f\"loaded {csv_file}\")\n",
    "        except Exception as e:\n",
    "            print(f\"error loading {csv_file}: {e}\")\n",
    "    \n",
    "    if not df_list:\n",
    "        print(\"no valid CSV files loaded\")\n",
    "        return False\n",
    "    \n",
    "    #combine all dataframes\n",
    "    df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"total annotations: {len(df)}\")\n",
    "    \n",
    "    #extract class names\n",
    "    try:\n",
    "        df['class'] = df['region_attributes'].apply(lambda x: json.loads(x)['candy_type'])\n",
    "        class_names = sorted(df['class'].unique())\n",
    "        class_to_id = {name: i for i, name in enumerate(class_names)}\n",
    "        \n",
    "        print(f\"found {len(class_names)} classes: {class_names}\")\n",
    "        \n",
    "        #write classes.txt\n",
    "        with open(\"classes.txt\", \"w\") as f:\n",
    "            for name in class_names:\n",
    "                f.write(name + \"\\n\")\n",
    "        \n",
    "        print(\"created classes.txt\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error extracting classes: {e}\")\n",
    "        return False\n",
    "    \n",
    "    #convert annotations to YOLO format\n",
    "    converted_count = 0\n",
    "    for filename in df['filename'].unique():\n",
    "        img_path = os.path.join(image_folder, filename)\n",
    "        label_path = os.path.join(output_labels, os.path.splitext(filename)[0] + '.txt')\n",
    "        \n",
    "        if not os.path.exists(img_path):\n",
    "            print(f\"image not found: {img_path}\")\n",
    "            continue\n",
    "        \n",
    "        try:\n",
    "            img = Image.open(img_path)\n",
    "            img_w, img_h = img.size\n",
    "        except Exception as e:\n",
    "            print(f\"failed to open image {filename}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        with open(label_path, \"w\") as f:\n",
    "            rows = df[df['filename'] == filename]\n",
    "            for _, row in rows.iterrows():\n",
    "                try:\n",
    "                    shape = json.loads(row['region_shape_attributes'])\n",
    "                    label = json.loads(row['region_attributes'])['candy_type']\n",
    "                    class_id = class_to_id[label]\n",
    "                    \n",
    "                    x, y, w, h = shape['x'], shape['y'], shape['width'], shape['height']\n",
    "                    x_center = (x + w / 2) / img_w\n",
    "                    y_center = (y + h / 2) / img_h\n",
    "                    w_norm = w / img_w\n",
    "                    h_norm = h / img_h\n",
    "                    \n",
    "                    f.write(f\"{class_id} {x_center:.6f} {y_center:.6f} {w_norm:.6f} {h_norm:.6f}\\n\")\n",
    "                except Exception as e:\n",
    "                    print(f\"skipping annotation in {filename}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        converted_count += 1\n",
    "    \n",
    "    print(f\"converted {converted_count} image-label pairs\")\n",
    "    return True\n",
    "\n",
    "#run the conversion\n",
    "if os.path.exists(\"annotation CSV files\"):\n",
    "    convert_csv_to_yolo()\n",
    "else:\n",
    "    print(\"no CSV annotations found - assuming labels already exist\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "29d46b27-bb19-4831-bde4-72f464e42475",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating train/validation split...\n",
      "found 528 images and 528 labels\n",
      "found 528 matching image-label pairs\n",
      "split: 475 training, 53 validation\n",
      "successfully created 475 training and 53 validation pairs\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_train_val_split(train_ratio=0.9):\n",
    "    \"\"\"create proper train/validation split\"\"\"\n",
    "    \n",
    "    print(\"creating train/validation split...\")\n",
    "    \n",
    "    #check source directories\n",
    "    if not os.path.exists('Images') or not os.path.exists('labels'):\n",
    "        print(\"Images or labels directory not found\")\n",
    "        return False\n",
    "    \n",
    "    #get all image files\n",
    "    image_files = []\n",
    "    for ext in ['.jpg', '.jpeg', '.JPG', '.JPEG']:\n",
    "        image_files.extend([f for f in os.listdir('Images') if f.endswith(ext)])\n",
    "    \n",
    "    #get all label files\n",
    "    label_files = [f for f in os.listdir('labels') if f.endswith('.txt')]\n",
    "    \n",
    "    print(f\"found {len(image_files)} images and {len(label_files)} labels\")\n",
    "    \n",
    "    #find matching pairs\n",
    "    matched_pairs = []\n",
    "    for img_file in image_files:\n",
    "        base_name = os.path.splitext(img_file)[0]\n",
    "        label_file = base_name + '.txt'\n",
    "        \n",
    "        if label_file in label_files:\n",
    "            matched_pairs.append((img_file, label_file))\n",
    "    \n",
    "    print(f\"found {len(matched_pairs)} matching image-label pairs\")\n",
    "    \n",
    "    if len(matched_pairs) == 0:\n",
    "        print(\"no matching pairs found\")\n",
    "        return False\n",
    "    \n",
    "    #split\n",
    "    random.seed(42)\n",
    "    random.shuffle(matched_pairs)\n",
    "    \n",
    "    split_idx = int(len(matched_pairs) * train_ratio)\n",
    "    train_pairs = matched_pairs[:split_idx]\n",
    "    val_pairs = matched_pairs[split_idx:]\n",
    "    \n",
    "    print(f\"split: {len(train_pairs)} training, {len(val_pairs)} validation\")\n",
    "    \n",
    "    #copy files to dataset structure\n",
    "    train_success = 0\n",
    "    val_success = 0\n",
    "    \n",
    "    for img_file, label_file in train_pairs:\n",
    "        try:\n",
    "            shutil.copy2(os.path.join('Images', img_file), \n",
    "                        os.path.join('dataset/images/train', img_file))\n",
    "            shutil.copy2(os.path.join('labels', label_file), \n",
    "                        os.path.join('dataset/labels/train', label_file))\n",
    "            train_success += 1\n",
    "        except Exception as e:\n",
    "            print(f\"failed to copy training pair {img_file}: {e}\")\n",
    "    \n",
    "    for img_file, label_file in val_pairs:\n",
    "        try:\n",
    "            shutil.copy2(os.path.join('Images', img_file), \n",
    "                        os.path.join('dataset/images/val', img_file))\n",
    "            shutil.copy2(os.path.join('labels', label_file), \n",
    "                        os.path.join('dataset/labels/val', label_file))\n",
    "            val_success += 1\n",
    "        except Exception as e:\n",
    "            print(f\"failed to copy validation pair {img_file}: {e}\")\n",
    "    \n",
    "    print(f\"successfully created {train_success} training and {val_success} validation pairs\")\n",
    "    return True\n",
    "\n",
    "#create the split\n",
    "create_train_val_split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f61b64d7-77d6-4405-9e24-ffec8781bf26",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "creating data.yaml...\n",
      "created data.yaml with 9 classes\n",
      "classes: ['100_Grand', '3_Musketeers', 'Baby_Ruth', 'Butterfingers', 'Crunch', 'Midnight_Milky_Way', 'Milky_Way', 'Snickers', 'Twix']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_data_yaml():\n",
    "    \"\"\"create data.yaml configuration file\"\"\"\n",
    "    \n",
    "    print(\"creating data.yaml...\")\n",
    "    \n",
    "    #read class names\n",
    "    try:\n",
    "        with open(\"classes.txt\", \"r\") as f:\n",
    "            class_names = [line.strip() for line in f.readlines() if line.strip()]\n",
    "    except FileNotFoundError:\n",
    "        print(\"classes.txt not found!\")\n",
    "        return False\n",
    "    \n",
    "    #create data.yaml content\n",
    "    data_content = {\n",
    "        'path': 'dataset',\n",
    "        'train': 'images/train',\n",
    "        'val': 'images/val',\n",
    "        'nc': len(class_names),\n",
    "        'names': class_names\n",
    "    }\n",
    "    \n",
    "    #write to file\n",
    "    with open('data.yaml', 'w') as f:\n",
    "        yaml.dump(data_content, f, default_flow_style=False, sort_keys=False)\n",
    "    \n",
    "    print(f\"created data.yaml with {len(class_names)} classes\")\n",
    "    print(f\"classes: {class_names}\")\n",
    "    return True\n",
    "\n",
    "#create data.yaml\n",
    "create_data_yaml()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4e9a30d-7197-4cb6-9cc1-f207f111452c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting model training...\n",
      "New https://pypi.org/project/ultralytics/8.3.168 available ðŸ˜ƒ Update with 'pip install -U ultralytics'\n",
      "Ultralytics 8.3.167 ðŸš€ Python-3.12.2 torch-2.7.1 CPU (Apple M1 Pro)\n",
      "\u001b[34m\u001b[1mengine/trainer: \u001b[0magnostic_nms=False, amp=True, augment=False, auto_augment=randaugment, batch=16, bgr=0.0, box=7.5, cache=False, cfg=None, classes=None, close_mosaic=10, cls=0.5, conf=None, copy_paste=0.0, copy_paste_mode=flip, cos_lr=False, cutmix=0.0, data=data.yaml, degrees=0.0, deterministic=True, device=cpu, dfl=1.5, dnn=False, dropout=0.0, dynamic=False, embed=None, epochs=20, erasing=0.4, exist_ok=False, fliplr=0.5, flipud=0.0, format=torchscript, fraction=1.0, freeze=None, half=False, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, imgsz=640, int8=False, iou=0.7, keras=False, kobj=1.0, line_width=None, lr0=0.01, lrf=0.01, mask_ratio=4, max_det=300, mixup=0.0, mode=train, model=yolov8n.pt, momentum=0.937, mosaic=1.0, multi_scale=False, name=candy_detection9, nbs=64, nms=False, opset=None, optimize=False, optimizer=auto, overlap_mask=True, patience=20, perspective=0.0, plots=True, pose=12.0, pretrained=True, profile=False, project=None, rect=False, resume=False, retina_masks=False, save=True, save_conf=False, save_crop=False, save_dir=/Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9, save_frames=False, save_json=False, save_period=-1, save_txt=False, scale=0.5, seed=0, shear=0.0, show=False, show_boxes=True, show_conf=True, show_labels=True, simplify=True, single_cls=False, source=None, split=val, stream_buffer=False, task=detect, time=None, tracker=botsort.yaml, translate=0.1, val=True, verbose=True, vid_stride=1, visualize=False, warmup_bias_lr=0.1, warmup_epochs=3.0, warmup_momentum=0.8, weight_decay=0.0005, workers=8, workspace=None\n",
      "Overriding model.yaml nc=80 with nc=9\n",
      "\n",
      "                   from  n    params  module                                       arguments                     \n",
      "  0                  -1  1       464  ultralytics.nn.modules.conv.Conv             [3, 16, 3, 2]                 \n",
      "  1                  -1  1      4672  ultralytics.nn.modules.conv.Conv             [16, 32, 3, 2]                \n",
      "  2                  -1  1      7360  ultralytics.nn.modules.block.C2f             [32, 32, 1, True]             \n",
      "  3                  -1  1     18560  ultralytics.nn.modules.conv.Conv             [32, 64, 3, 2]                \n",
      "  4                  -1  2     49664  ultralytics.nn.modules.block.C2f             [64, 64, 2, True]             \n",
      "  5                  -1  1     73984  ultralytics.nn.modules.conv.Conv             [64, 128, 3, 2]               \n",
      "  6                  -1  2    197632  ultralytics.nn.modules.block.C2f             [128, 128, 2, True]           \n",
      "  7                  -1  1    295424  ultralytics.nn.modules.conv.Conv             [128, 256, 3, 2]              \n",
      "  8                  -1  1    460288  ultralytics.nn.modules.block.C2f             [256, 256, 1, True]           \n",
      "  9                  -1  1    164608  ultralytics.nn.modules.block.SPPF            [256, 256, 5]                 \n",
      " 10                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 11             [-1, 6]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 12                  -1  1    148224  ultralytics.nn.modules.block.C2f             [384, 128, 1]                 \n",
      " 13                  -1  1         0  torch.nn.modules.upsampling.Upsample         [None, 2, 'nearest']          \n",
      " 14             [-1, 4]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 15                  -1  1     37248  ultralytics.nn.modules.block.C2f             [192, 64, 1]                  \n",
      " 16                  -1  1     36992  ultralytics.nn.modules.conv.Conv             [64, 64, 3, 2]                \n",
      " 17            [-1, 12]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 18                  -1  1    123648  ultralytics.nn.modules.block.C2f             [192, 128, 1]                 \n",
      " 19                  -1  1    147712  ultralytics.nn.modules.conv.Conv             [128, 128, 3, 2]              \n",
      " 20             [-1, 9]  1         0  ultralytics.nn.modules.conv.Concat           [1]                           \n",
      " 21                  -1  1    493056  ultralytics.nn.modules.block.C2f             [384, 256, 1]                 \n",
      " 22        [15, 18, 21]  1    753067  ultralytics.nn.modules.head.Detect           [9, [64, 128, 256]]           \n",
      "Model summary: 129 layers, 3,012,603 parameters, 3,012,587 gradients, 8.2 GFLOPs\n",
      "\n",
      "Transferred 319/355 items from pretrained weights\n",
      "Freezing layer 'model.22.dfl.conv.weight'\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 3125.9Â±960.6 MB/s, size: 5718.6 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /Users/larasabha/Desktop/Candy-object-detection/dataset/labels/train.cache... 475 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 475/475 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mFast image access âœ… (ping: 0.0Â±0.0 ms, read: 4082.0Â±1575.7 MB/s, size: 5856.3 KB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /Users/larasabha/Desktop/Candy-object-detection/dataset/labels/val.cache... 53 images, 0 backgrounds, 0 corrupt: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 53/53 [00:00<?, ?it/s]\u001b[0m"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting labels to /Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9/labels.jpg... \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1moptimizer:\u001b[0m 'optimizer=auto' found, ignoring 'lr0=0.01' and 'momentum=0.937' and determining best 'optimizer', 'lr0' and 'momentum' automatically... \n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m AdamW(lr=0.000769, momentum=0.9) with parameter groups 57 weight(decay=0.0), 64 weight(decay=0.0005), 63 bias(decay=0.0)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 0 dataloader workers\n",
      "Logging results to \u001b[1m/Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9\u001b[0m\n",
      "Starting training for 20 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "       1/20         0G     0.7768      3.222       1.09         78        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:35<00:00,  7.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212     0.0174          1      0.311      0.291\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       2/20         0G     0.6932      2.037      1.043        111        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:34<00:00,  7.16s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.757      0.553      0.594       0.55\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       3/20         0G     0.6505      1.237      1.043         85        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:39<00:00,  7.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212       0.58      0.748      0.751      0.678\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       4/20         0G     0.6026     0.9324      1.006        100        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:36<00:00,  7.21s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.96s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.941      0.842      0.954      0.881\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       5/20         0G     0.5798     0.8182      1.001        101        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:41<00:00,  7.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.975      0.985      0.995      0.933\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       6/20         0G      0.567     0.7597     0.9891        125        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:49<00:00,  7.64s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.83s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.987      0.995      0.995      0.922\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       7/20         0G     0.5725      0.729      0.985         90        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:49<00:00,  7.65s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.979      0.992      0.995      0.941\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       8/20         0G     0.5292     0.6688     0.9588         97        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:41<00:00,  7.40s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.995      0.996      0.995      0.947\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "       9/20         0G     0.5195      0.647     0.9631         83        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:51<00:00,  7.73s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.994      0.996      0.995      0.943\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      10/20         0G     0.5058     0.6281     0.9521        102        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:47<00:00,  7.58s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.985      0.996      0.995      0.947\n",
      "Closing dataloader mosaic\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      11/20         0G     0.3748     0.5995     0.8618         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:44<00:00,  7.48s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.81s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.984      0.985      0.995      0.953\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      12/20         0G     0.3506     0.5444     0.8418         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:41<00:00,  7.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:11<00:00,  5.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.995      0.999      0.995      0.945\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      13/20         0G     0.3423     0.5252     0.8441         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:35<00:00,  7.19s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.11s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.995      0.999      0.995      0.946\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      14/20         0G     0.3354     0.4953     0.8388         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:14<00:00,  6.49s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.16s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.995          1      0.995      0.953\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      15/20         0G       0.33     0.4716      0.837         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:11<00:00,  6.37s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.996          1      0.995      0.958\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "      16/20         0G     0.3273     0.4608     0.8397         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:09<00:00,  6.32s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.996      0.998      0.995      0.949\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      17/20         0G     0.3196     0.4429      0.832         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:08<00:00,  6.29s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.25s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.996      0.997      0.995      0.954\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      18/20         0G     0.3164     0.4332     0.8283         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:09<00:00,  6.33s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.997          1      0.995      0.955\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      19/20         0G     0.3059     0.4208     0.8277         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:08<00:00,  6.30s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.27s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.996          1      0.995       0.96\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   cls_loss   dfl_loss  Instances       Size\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "      20/20         0G     0.3026     0.4115     0.8259         44        640: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 30/30 [03:08<00:00,  6.28s/it]\n",
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.13s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.997          1      0.995      0.961\n",
      "\n",
      "20 epochs completed in 1.235 hours.\n",
      "Optimizer stripped from /Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9/weights/last.pt, 6.2MB\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizer stripped from /Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9/weights/best.pt, 6.2MB\n",
      "\n",
      "Validating /Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9/weights/best.pt...\n",
      "Ultralytics 8.3.167 ðŸš€ Python-3.12.2 torch-2.7.1 CPU (Apple M1 Pro)\n",
      "Model summary (fused): 72 layers, 3,007,403 parameters, 0 gradients, 8.1 GFLOPs\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                 Class     Images  Instances      Box(P          R      mAP50  mAP50-95): 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:10<00:00,  5.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                   all         53        212      0.997          1      0.995      0.961\n",
      "             100_Grand         26         26      0.997          1      0.995      0.976\n",
      "          3_Musketeers         26         26      0.998          1      0.995      0.918\n",
      "             Baby_Ruth         21         21      0.997          1      0.995      0.984\n",
      "         Butterfingers         24         24      0.996          1      0.995      0.984\n",
      "                Crunch         26         26      0.998          1      0.995      0.983\n",
      "    Midnight_Milky_Way         20         20      0.998          1      0.995      0.931\n",
      "             Milky_Way         24         24      0.997          1      0.995      0.951\n",
      "              Snickers         23         23      0.998          1      0.995      0.953\n",
      "                  Twix         22         22      0.996          1      0.995      0.969\n",
      "Speed: 0.8ms preprocess, 107.4ms inference, 0.0ms loss, 1.9ms postprocess per image\n",
      "Results saved to \u001b[1m/Users/larasabha/PycharmProjects/Candy-object-detection/runs/detect/candy_detection9\u001b[0m\n",
      "training completed\n",
      "results saved in: runs/detect/candy_detection/\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train_model(epochs=100, img_size=640, batch_size=16):\n",
    "    \"\"\"train YOLO model\"\"\"\n",
    "    \n",
    "    print(\"starting model training...\")\n",
    "    \n",
    "    #check if data.yaml exists\n",
    "    if not os.path.exists('data.yaml'):\n",
    "        print(\"data.yaml not found. Run data preparation first.\")\n",
    "        return False\n",
    "    \n",
    "    #load base model\n",
    "    model = YOLO('yolov8n.pt')  #download base model automatically\n",
    "    \n",
    "    #start training\n",
    "    try:\n",
    "        results = model.train(\n",
    "            data='data.yaml',\n",
    "            epochs=epochs,\n",
    "            imgsz=img_size,\n",
    "            batch=batch_size,\n",
    "            name='candy_detection',\n",
    "            patience=20,\n",
    "            save=True,\n",
    "            plots=True\n",
    "        )\n",
    "        \n",
    "        print(\"training completed\")\n",
    "        print(f\"results saved in: runs/detect/candy_detection/\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"training failed: {e}\")\n",
    "        return False\n",
    "\n",
    "#start training\n",
    "train_model(epochs=20)  # reduced epochs for faster training -> change to 100 for complete training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "1df3ee6f-e90d-41fb-9424-35fac6b816a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "found model: runs/detect/train2/weights/best.pt\n"
     ]
    }
   ],
   "source": [
    "#check for existing trained model\n",
    "def find_best_model():\n",
    "    \"\"\"find the best trained model\"\"\"\n",
    "    \n",
    "    possible_paths = [\n",
    "        \"runs/detect/candy_detection/weights/best.pt\",\n",
    "        \"runs/detect/train/weights/best.pt\",\n",
    "        \"runs/detect/train2/weights/best.pt\",\n",
    "        \"best.pt\",\n",
    "        \"my_model.pt\"\n",
    "    ]\n",
    "    \n",
    "    for path in possible_paths:\n",
    "        if os.path.exists(path):\n",
    "            print(f\"found model: {path}\")\n",
    "            return path\n",
    "    \n",
    "    print(\"no trained model found. Please train first or provide model path.\")\n",
    "    return None\n",
    "\n",
    "model_path = find_best_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9c7375d8-cc78-4bcc-9e33-2d45f1a289f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing detection on: /Users/larasabha/Desktop/image.jpg\n",
      "model loaded: runs/detect/train2/weights/best.pt\n",
      "image size: 225x225\n",
      "\n",
      "0: 640x640 1 Milky_Way, 1 Snickers, 56.5ms\n",
      "Speed: 1.2ms preprocess, 56.5ms inference, 0.7ms postprocess per image at shape (1, 3, 640, 640)\n",
      "found 2 detections:\n",
      "  1. Snickers (0.98)\n",
      "  2. Milky_Way (0.72)\n",
      "result saved to: test_results/detection_result.jpg\n"
     ]
    }
   ],
   "source": [
    "def test_single_image(image_path, model_path=None, conf_threshold=0.5):\n",
    "    \"\"\"test detection on a single image\"\"\"\n",
    "    \n",
    "    print(f\"testing detection on: {image_path}\")\n",
    "    \n",
    "    #find model if not provided\n",
    "    if model_path is None:\n",
    "        model_path = find_best_model()\n",
    "        if model_path is None:\n",
    "            return False\n",
    "    \n",
    "    #check image exists\n",
    "    if not os.path.exists(image_path):\n",
    "        print(f\"image not found: {image_path}\")\n",
    "        return False\n",
    "    \n",
    "    #load model\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"model loaded: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error loading model: {e}\")\n",
    "        return False\n",
    "    \n",
    "    #load image\n",
    "    try:\n",
    "        image = cv2.imread(image_path)\n",
    "        if image is None:\n",
    "            print(\"could not load image\")\n",
    "            return False\n",
    "        \n",
    "        height, width = image.shape[:2]\n",
    "        print(f\"image size: {width}x{height}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error loading image: {e}\")\n",
    "        return False\n",
    "    \n",
    "    #run detection\n",
    "    try:\n",
    "        results = model(image, conf=conf_threshold)\n",
    "        \n",
    "        detections = []\n",
    "        result_image = image.copy()\n",
    "        \n",
    "        for result in results:\n",
    "            if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "                for box in result.boxes:\n",
    "                    if box.xyxy is None or box.conf is None or box.cls is None:\n",
    "                        continue\n",
    "                    \n",
    "                    #get detection\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0].cpu().numpy())\n",
    "                    conf = float(box.conf[0].cpu().numpy())\n",
    "                    class_id = int(box.cls[0].cpu().numpy())\n",
    "                    class_name = model.names.get(class_id, f\"Class_{class_id}\")\n",
    "                    \n",
    "                    detections.append({\n",
    "                        'class': class_name,\n",
    "                        'confidence': conf,\n",
    "                        'box': [x1, y1, x2, y2]\n",
    "                    })\n",
    "                    \n",
    "                    #draw on image\n",
    "                    color = (0, 255, 0)  # Green\n",
    "                    cv2.rectangle(result_image, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(result_image, f\"{class_name} {conf:.2f}\", \n",
    "                              (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, \n",
    "                              0.6, color, 2)\n",
    "        \n",
    "        print(f\"found {len(detections)} detections:\")\n",
    "        for i, det in enumerate(detections):\n",
    "            print(f\"  {i+1}. {det['class']} ({det['confidence']:.2f})\")\n",
    "        \n",
    "        #save result\n",
    "        output_path = \"test_results/detection_result.jpg\"\n",
    "        cv2.imwrite(output_path, result_image)\n",
    "        print(f\"result saved to: {output_path}\")\n",
    "        \n",
    "        return True, detections, result_image\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"error during detection: {e}\")\n",
    "        return False, [], None\n",
    "\n",
    "#test with desktop image (change path as needed)\n",
    "desktop_image = str(Path.home() / \"Desktop\" / \"image.jpg\")\n",
    "if os.path.exists(desktop_image):\n",
    "    test_single_image(desktop_image, model_path)\n",
    "else:\n",
    "    print(f\"no test image found at {desktop_image}\")\n",
    "    print(\"place an image named 'image.jpg' on your Desktop to test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "7135d21e-80e9-4049-809d-6f874fdbceae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "testing detection on images in: Images\n",
      "model loaded: runs/detect/train2/weights/best.pt\n",
      "found 528 images\n",
      "\n",
      "0: 480x640 1 Butterfingers, 1 Midnight_Milky_Way, 1 Snickers, 1 Twix, 55.6ms\n",
      "Speed: 5.6ms preprocess, 55.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2165.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 3_Musketeers, 1 Butterfingers, 1 Midnight_Milky_Way, 1 Snickers, 46.5ms\n",
      "Speed: 2.8ms preprocess, 46.5ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2171.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 Butterfingers, 1 Milky_Way, 1 Snickers, 1 Twix, 50.2ms\n",
      "Speed: 2.7ms preprocess, 50.2ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2159.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 Crunch, 1 Milky_Way, 1 Snickers, 1 Twix, 45.6ms\n",
      "Speed: 2.2ms preprocess, 45.6ms inference, 1.3ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2398.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 Crunch, 1 Midnight_Milky_Way, 1 Snickers, 1 Twix, 50.7ms\n",
      "Speed: 2.4ms preprocess, 50.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2401.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 100_Grand, 1 Crunch, 1 Milky_Way, 1 Twix, 49.2ms\n",
      "Speed: 3.3ms preprocess, 49.2ms inference, 1.0ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2367.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 100_Grand, 1 Crunch, 1 Midnight_Milky_Way, 1 Twix, 43.9ms\n",
      "Speed: 2.2ms preprocess, 43.9ms inference, 0.9ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2373.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 Crunch, 1 Midnight_Milky_Way, 1 Milky_Way, 1 Snickers, 45.7ms\n",
      "Speed: 2.5ms preprocess, 45.7ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2415.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 Crunch, 1 Midnight_Milky_Way, 1 Milky_Way, 1 Twix, 45.7ms\n",
      "Speed: 2.0ms preprocess, 45.7ms inference, 0.6ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2429.JPG: 4 detections\n",
      "\n",
      "0: 480x640 1 100_Grand, 1 Baby_Ruth, 1 Crunch, 1 Midnight_Milky_Way, 51.9ms\n",
      "Speed: 3.1ms preprocess, 51.9ms inference, 0.7ms postprocess per image at shape (1, 3, 480, 640)\n",
      "IMG_2213.JPG: 4 detections\n",
      "summary: 40 total detections in 10 images\n"
     ]
    }
   ],
   "source": [
    "def test_multiple_images(image_folder=\"Images\", model_path=None, conf_threshold=0.5):\n",
    "    \"\"\"test detection on multiple images\"\"\"\n",
    "    \n",
    "    print(f\"testing detection on images in: {image_folder}\")\n",
    "    \n",
    "    if model_path is None:\n",
    "        model_path = find_best_model()\n",
    "        if model_path is None:\n",
    "            return False\n",
    "    \n",
    "    #load model\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        print(f\"model loaded: {model_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"error loading model: {e}\")\n",
    "        return False\n",
    "    \n",
    "    #get all image files\n",
    "    image_files = []\n",
    "    for ext in ['.jpg', '.jpeg', '.JPG', '.JPEG']:\n",
    "        image_files.extend([f for f in os.listdir(image_folder) if f.endswith(ext)])\n",
    "    \n",
    "    if not image_files:\n",
    "        print(f\"no images found in {image_folder}\")\n",
    "        return False\n",
    "    \n",
    "    print(f\"found {len(image_files)} images\")\n",
    "    \n",
    "    total_detections = 0\n",
    "    processed_count = 0\n",
    "    \n",
    "    for img_file in image_files[:10]:  # test first 10 images\n",
    "        img_path = os.path.join(image_folder, img_file)\n",
    "        \n",
    "        try:\n",
    "            image = cv2.imread(img_path)\n",
    "            if image is None:\n",
    "                continue\n",
    "            \n",
    "            results = model(image, conf=conf_threshold)\n",
    "            \n",
    "            detections = 0\n",
    "            for result in results:\n",
    "                if hasattr(result, 'boxes') and result.boxes is not None:\n",
    "                    detections += len(result.boxes)\n",
    "            \n",
    "            total_detections += detections\n",
    "            processed_count += 1\n",
    "            \n",
    "            if detections > 0:\n",
    "                print(f\"{img_file}: {detections} detections\")\n",
    "        \n",
    "        except Exception as e:\n",
    "            print(f\"error processing {img_file}: {e}\")\n",
    "    \n",
    "    print(f\"summary: {total_detections} total detections in {processed_count} images\")\n",
    "    return True\n",
    "\n",
    "# test on multiple images\n",
    "if os.path.exists(\"Images\"):\n",
    "    test_multiple_images(\"Images\", model_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Candy Detector Env",
   "language": "python",
   "name": "my_candy_detector"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
